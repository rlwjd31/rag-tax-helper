{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "885be740",
   "metadata": {},
   "source": [
    "# LangChain Basic\n",
    "\n",
    "ollamaì„ ì œì™¸í•œ ìƒìš© AIë“¤ì€ ë¹„ìš©ì´ ë“ ë‹¤.  \n",
    "\n",
    "ë”°ë¼ì„œ, ë¹„ìš©ì´ ì—†ì´ LLMì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” open sourceë¡œ ì œê³µë˜ëŠ” ollamaë¥¼ í™œìš©í•˜ì—¬ ì§„í–‰í•  ìˆ˜ ìˆì§€ë§Œ, ì„±ëŠ¥ì€ ìƒìš© LLMë³´ë‹¨ ì¡°ê¸ˆ ë–¨ì–´ì§„ë‹¤ê³  í•œë‹¤.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0eec1f",
   "metadata": {},
   "source": [
    "# 1. Install Ollama[(ğŸ”—Link)[https://ollama.com/]]\n",
    "\n",
    "(ğŸ”—í•´ë‹¹ ë§í¬)[https://ollama.com/]ë¥¼ í†µí•´ì„œ ollamaë¥¼ ì„¤ì¹˜í•œë‹¤.\n",
    "\n",
    "ollamaì—ì„œ ì œê³µí•˜ëŠ” ìˆ˜ë§ì€ LLM modelì´ ìˆëŠ”ë° ë‚œ ê°•ì˜ì—ì„œ ì‚¬ìš©í•˜ëŠ” deepseek-r1ì•„ ì•„ë‹Œ metaì—ì„œ 2024ë…„ 12ì›”ì— ë°œí‘œí•œ `llama3.3`ì„ ì‚¬ìš©í•˜ë ¤ê³  í–ˆì§€ë§Œ ìš©ëŸ‰ì´ í¬ê³  textë¥¼ ìœ„ì£¼ë¡œ í•  ê²ƒì´ê¸° ë•Œë¬¸ì— `3.1`ë¥¼ ì‚¬ìš©í•œë‹¤.\n",
    "\n",
    "\n",
    "ollama -> models -> llama3.1ì„ ì°¾ì•„ì„œ ì •ë³´ë¥¼ í™•ì¸í›„ ì•„ë˜ commandë¥¼ í†µí•´ì„œ localì— ë‹¤ìš´ë°›ëŠ”ë‹¤.\n",
    "```shell\n",
    "$) ollama pull llama3.1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463467fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install -Uq langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "164fa85d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ì„œìš¸ì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-07-20T06:01:49.508797Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1170241542, 'load_duration': 767241209, 'prompt_eval_count': 17, 'prompt_eval_duration': 340222541, 'eval_count': 4, 'eval_duration': 61469375, 'model_name': 'llama3.1'}, id='run--0bcc0468-0a95-47cf-8020-cbb1dd67d991-0', usage_metadata={'input_tokens': 17, 'output_tokens': 4, 'total_tokens': 21})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1\")\n",
    "llm.invoke(\"ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ ì•Œë ¤ì¤˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30812a0b",
   "metadata": {},
   "source": [
    "# 2. prompt\n",
    "\n",
    "ìœ„ì—ì„œ llm.invokeì— ë“¤ì–´ê°€ëŠ” ì¸ìëŠ” `prompt`ë¡œì„œ llmì— ì§ˆì˜í•˜ê¸° ìœ„í•œ interfaceë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤.   \n",
    "promptë¡œ ë“¤ì–´ê°ˆ ìˆ˜ ìˆëŠ” typeì€ ì•„ë˜ `3ê°€ì§€`ì´ë‹¤.\n",
    "\n",
    "1. Prompt Value => `langchainì˜ prompt template`ì„ ì‚¬ìš©í•˜ì—¬ ë§Œë“¤ ìˆ˜ ìˆìŒ\n",
    "2. string\n",
    "3. list of BaseMessage  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4b79b4",
   "metadata": {},
   "source": [
    "## 2.1 Prompt Valueë¥¼ í†µí•´ ì§ˆì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9bf5cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='There are actually two countries called \"Korea\": North Korea (Democratic People\\'s Republic of Korea) and South Korea (Republic of Korea).\\n\\nThe capital of:\\n\\n* **North Korea** is Pyongyang.\\n* **South Korea** is Seoul.\\n\\nSo, which one were you thinking of?', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-07-20T05:56:13.931602Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1365230625, 'load_duration': 80469584, 'prompt_eval_count': 17, 'prompt_eval_duration': 156762875, 'eval_count': 58, 'eval_duration': 1127568667, 'model_name': 'llama3.1'}, id='run--9e2c5e7c-71a7-4f88-863a-f8b1c263fd84-0', usage_metadata={'input_tokens': 17, 'output_tokens': 58, 'total_tokens': 75})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# * \"country\"ë¼ëŠ” ë³€ìˆ˜ë¥¼ ë°›ì•„ì„œ templateì— ìˆëŠ” countryë¡œ ë„£ì–´ì¤€ë‹¤.\n",
    "# * ì•„ë˜ template.invokeë¥¼ í†µí•´ì„œ ë“¤ì–´ê°„ë‹¤.\n",
    "prompt_template = PromptTemplate(\n",
    "  template=\"What is the capital of {country}?\",\n",
    "  input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.invoke({\"country\": \"Korea\"})\n",
    "llm = ChatOllama(model=\"llama3.1\")\n",
    "llm.invoke(input=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954b890a",
   "metadata": {},
   "source": [
    "## 2.2 List of BaseMessageë¥¼ í†µí•œ ì§ˆì˜\n",
    "\n",
    "BaseMessageë¥¼ ìƒì†í•˜ëŠ” í´ë˜ìŠ¤ë“¤ì´ ìˆëŠ”ë° `4 ê°€ì§€`ê°€ ì¡´ì¬í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.\n",
    "\n",
    "1. HumanMessage => ì‚¬ìš©ìì˜ ì§ˆì˜\n",
    "2. AIMessage => LLMì˜ ë‹µë³€\n",
    "3. SystemMessage => prompt engineeringì—ì„œ í˜ë¥´ì†Œë‚˜ ë•Œ ì‚¬ìš©\n",
    "4. ToolMessage => AI agentì—ì„œ ì‚¬ìš©\n",
    "\n",
    "ì•„ë˜ì—ì„œ `prompt engineering` ê¸°ë²• ì¤‘ few-shots, zero-shot, one-shotê³¼ ê°™ì€ ê¸°ë²•ì´ ë‚˜ì˜¤ëŠ”ë° ì—¬ê¸°ì„œ `shot = ì˜ˆì‹œ ë˜ëŠ” ì˜ˆì œ; ì¦‰, ì§ˆë¬¸-ë‹µë³€ í•œ ìŒ`ì´ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤.  \n",
    "\n",
    "ì•„ë˜ [ğŸ”—ë…¼ë¬¸](https://arxiv.org/abs/2005.14165)ì—ì„œ ë°œì·Œí•œ ì´ë¯¸ì§€ì´ë‹¤.  \n",
    "\n",
    "![what is shot](./few-shots.png)\n",
    "\n",
    "ìœ„ì˜ ì´ë¯¸ì§€ì—ì„œ ì˜ˆì‹œë¥¼ ë§ì´ì£¼ë©´ `ë‹µë³€ì˜ ì •í™•ë„ê°€ ë†’ì•„ì§`ì„ ë…¼ë¬¸ì—ì„œ ë°œí‘œí•œë‹¤.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0286f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ë„¤, í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸íŠ¹ë³„ì‹œì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-07-20T06:19:35.103928Z', 'done': True, 'done_reason': 'stop', 'total_duration': 406587375, 'load_duration': 33495375, 'prompt_eval_count': 69, 'prompt_eval_duration': 193075209, 'eval_count': 10, 'eval_duration': 178859416, 'model_name': 'llama3.1'}, id='run--8d686e25-d5a5-4d92-98ba-6747ede5b642-0', usage_metadata={'input_tokens': 69, 'output_tokens': 10, 'total_tokens': 79})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# * BaseMessageë¥¼ ìƒì†í•˜ëŠ” HumanMessageë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆì˜\n",
    "base_msg_result = llm.invoke([HumanMessage(content=\"ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?\")])\n",
    "\n",
    "# * BaseMessageë¥¼ ìƒì†í•˜ëŠ” ê²ƒì„ ë„£ì–´ì¤„ ìˆ˜ ìˆë‹¤.\n",
    "# ! ì•„ë˜ì—ì„œ HumanMessageê°€ ë§ˆì§€ë§‰ì— ì™€ì•¼ ì§ˆì˜ì— ëŒ€í•œ ì‘ë‹µì„ ì–»ì„ ìˆ˜ ìˆë‹¤.\n",
    "# ! AIMessageê°€ ë§ˆì§€ë§‰ì— ì˜¤ë©´ ê²°êµ­ ë‹µë³€ì´ê¸° ë•Œë¬¸ì— ë‹µë³€ì„ ë°›ì„ ìˆ˜ ì—†ë‹¤.\n",
    "# ? ì•„ë˜ì™€ ê°™ì´ SystemMessage, HumanMessage, AIMessageë¥¼ ë„£ëŠ” ì´ìœ ëŠ” AIì—ê²Œ ì§ˆì˜ì— ëŒ€í•œ historyê°€ ìˆëŠ” ê²ƒì²˜ëŸ¼ ì†ì´ê¸° ìœ„í•¨ì´ë‹¤.\n",
    "# ? ì´ë¥¼ í†µí•´ì„œ ë‹µë³€ì„ ìš°ë¦¬ê°€ ì›í•˜ëŠ”ëŒ€ë¡œ ìœ ë„í•  ìˆ˜ ìˆë‹¤.\n",
    "# ? ì•„ë˜ëŠ” ì§ˆë¬¸-ë‹µë³€ì´ í•œ ìŒì´ ì¡´ì¬í•˜ë¯€ë¡œ \"one-shot\"ì´ë‹¤.\n",
    "prompt_msg_list = [\n",
    "  SystemMessage(content=\"ë‹¹ì‹ ì€ ì„¸ê³„ì—ì„œ ìµœê³  ìœ ëª…í•œ ì§€ë¦¬í•™ìì´ì ë°•ì‚¬ì…ë‹ˆë‹¤.\"),\n",
    "  HumanMessage(content=\"ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?\"),\n",
    "  AIMessage(content=\"ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤.\"),\n",
    "  HumanMessage(content=\"ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?\"),  \n",
    "]\n",
    "\n",
    "llm.invoke(prompt_msg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba87661a",
   "metadata": {},
   "source": [
    "### ë©”ì„¸ì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„  `ChatPromptTemplate`ë¥¼ ì‚¬ìš©\n",
    "\n",
    "ìœ„ì˜ ì½”ë“œë³´ë‹¤ ì¶”í›„ `LCEL` íŒŒì´í”„ë¼ì¸ ê¸°ë²•ì— ì‚¬ìš©í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ  \n",
    "_í™•ì¥ì„± ì¸¡ë©´ì—ì„œ_ ì•„ë˜ì™€ ê°™ì´ `ChatPromptTemplate`ë¥¼ í†µí•œ message_listë¥¼ êµ¬ì„±í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b3b2250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_prompt_template_result => messages=[AIMessage(content='ë‹¹ì‹ ì€ ì„¸ê³„ì—ì„œ ìµœê³  ìœ ëª…í•œ ì§€ë¦¬í•™ìì´ì ë°•ì‚¬ì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}), HumanMessage(content='ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?', additional_kwargs={}, response_metadata={}), AIMessage(content='ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}), HumanMessage(content='ëŒ€í•œë¯¼êµ­ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?', additional_kwargs={}, response_metadata={})]\n",
      "result => content='ì„œìš¸ì…ë‹ˆë‹¤.' additional_kwargs={} response_metadata={'model': 'llama3.1', 'created_at': '2025-07-20T06:26:18.488727Z', 'done': True, 'done_reason': 'stop', 'total_duration': 301085666, 'load_duration': 79030541, 'prompt_eval_count': 68, 'prompt_eval_duration': 158884250, 'eval_count': 4, 'eval_duration': 61395750, 'model_name': 'llama3.1'} id='run--0304c60f-41c8-47b5-a9cc-2ba2f5cc0aa8-0' usage_metadata={'input_tokens': 68, 'output_tokens': 4, 'total_tokens': 72}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# ! ìœ„ì—ì„œì²˜ëŸ¼ ë¦¬ìŠ¤íŠ¸ì˜ ìš”ì†Œê°€ Human, AI, Systemì´ ì•„ë‹Œ \"tuple\"ë¡œ ë„£ì–´ì£¼ì–´ì•¼ countryê°€ ì‚¬ìš©ìì˜ ê°’ìœ¼ë¡œ ëŒ€ì²´ê°€ ëœë‹¤.\n",
    "base_msg_list = [\n",
    "  (\"assistant\", \"ë‹¹ì‹ ì€ ì„¸ê³„ì—ì„œ ìµœê³  ìœ ëª…í•œ ì§€ë¦¬í•™ìì´ì ë°•ì‚¬ì…ë‹ˆë‹¤.\"),\n",
    "  (\"human\", \"ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?\"),\n",
    "  (\"ai\", \"ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤.\"),\n",
    "  (\"human\", \"{country} ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?\"),\n",
    "]\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(base_msg_list)\n",
    "\n",
    "chat_prompt_template_result = chat_prompt_template.invoke({\"country\": \"ëŒ€í•œë¯¼êµ­\"})\n",
    "print(\"chat_prompt_template_result =>\", chat_prompt_template_result)\n",
    "result = llm.invoke(chat_prompt_template_result)\n",
    "print(\"result =>\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcfbb49",
   "metadata": {},
   "source": [
    "# pydanticê³¼ ì—°ê³„\n",
    "=> `section4(6:30)`ì—ì„œ json output parserë¥¼ ì‚¬ìš©í•´ë„ ì œëŒ€ë¡œ ëœ jsonì´ ë‚˜ì˜¤ì§€ ì•Šì•„ `pydantic`ê³¼ ì—°ê³„í•˜ì—¬ `output based schema with pydantic`ì„ í•  ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class CountryDetail(BaseModel):\n",
    "  capital: str = Field(description=\"The capital of the country\")\n",
    "  population: int = Field(description=\"The population of the country\")\n",
    "  language: str = Field(description=\"The language of the country\")\n",
    "  currency: str = Field(description=\"The currency of the country\")\n",
    "\n",
    "county_detail_prompt = PromptTemplate(\n",
    "  template=\"\"\"\n",
    "  Give following information about {country}:\n",
    "  - Capital\n",
    "  - Population\n",
    "  - Language\n",
    "  - Currency\n",
    "\n",
    "  return it in JSON format. and return the JSON dictionary only.\n",
    "  \"\"\",\n",
    "  input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "# ! json ouput parser ëŒ€ì‹  pydanticê³¼ with_structured_outputì„ ì—°ê³„í•˜ì—¬ì‘ì„±í•˜ì\n",
    "structured_llm = llm.with_structured_output(CountryDetail)\n",
    "json_ai_message = structured_llm.invoke(county_detail_prompt.invoke({\"country\": \"France\"}))\n",
    "\n",
    "# ì´ì™€ ê°™ì´ í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ ê° propertyì— ì‰½ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤.\n",
    "json_ai_message.capital\n",
    "json_ai_message.population\n",
    "json_ai_message.language\n",
    "json_ai_message.currency\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1497b584",
   "metadata": {},
   "source": [
    "# chainë¼ë¦¬ ì—°ê²°\n",
    "\n",
    "ìµœì¢… chainì€ ì‚¬ìš©ìë¡œë¶€í„° ë°›ì€ êµ­ê°€ ì´ë¦„ì„ í†µí•´ êµ­ê°€ì˜ ì •í˜•í™”ëœ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³  ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‚˜ë¼ êµ­ê¸°ì˜ ìƒ‰ê¹”ì„ ì¶”ë¡ í•˜ëŠ” chainì´ë‹¤.\n",
    "\n",
    "> final_chain = country_llm_chain(input_variables=[\"country\"]) => country_flag_colors_chain(input_variables=[\"information\"])\n",
    "\n",
    "- country_llm_chainì€ {\"country\": \"France\"}ì™€ ê°™ì´ ë„£ì–´ì£¼ì–´ì•¼ í•¨\n",
    "- country_flag_colors_chainì€ {\"information\": f\"{information}\"}ì™€ ê°™ì´ ë„£ì–´ì£¼ì–´ì•¼ í•¨\n",
    "\n",
    "`RunnablePassthrough`ë¥¼ í†µí•´ì„œ keyê°€ ì—†ì´ ë°”ë¡œ ê°’ì„ ë„£ì–´ì£¼ê³  `RunnableLambda`ë¥¼ í†µí•´ì„œ ì¤‘ê°„ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ëŠ” custom function pipelineì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b840c6",
   "metadata": {},
   "source": [
    "## country_llm_chain êµ¬í˜„\n",
    "\n",
    "`pydantic`ê³¼ `with_structured_output`ì„ í†µí•´ì„œ êµ­ê°€ì˜ ì •í˜•í™”ëœ ë°ì´í„°ë¥¼ ì¶”ì¶œí•œë‹¤.\n",
    "- ìˆ˜ë„\n",
    "- ì¸êµ¬\n",
    "- ì–¸ì–´\n",
    "- í†µí™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f47eb5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "template = \"\"\"\n",
    "  Give following information about {country}:\n",
    "  - Capital\n",
    "  - Population\n",
    "  - Language\n",
    "  - Currency\n",
    "\n",
    "  return it in JSON format. and return the JSON dictionary only.\n",
    "  \"\"\"\n",
    "\n",
    "class CountryDetail(BaseModel):\n",
    "  capital: str = Field(description=\"The capital of the country\")\n",
    "  population: int = Field(description=\"The population of the country\")\n",
    "  language: str = Field(description=\"The language of the country\")\n",
    "  currency: str = Field(description=\"The currency of the country\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\").with_structured_output(CountryDetail)\n",
    "prompt = PromptTemplate(\n",
    "  template=template,\n",
    "  input_variables=[\"country\"]  \n",
    ")\n",
    "\n",
    "country_llm_chain = prompt | llm\n",
    "\n",
    "# ! ì¤‘ê°„ test\n",
    "# answer = country_llm_chain.invoke({\"country\": \"France\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65437837",
   "metadata": {},
   "source": [
    "## country_flag_colors_chain êµ¬í˜„\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb37762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "capital_template = PromptTemplate(\n",
    "  template=\"\"\"\n",
    "  guess country's flag colors.\n",
    "  just return the colors.\n",
    "\n",
    "  # information\n",
    "  {information}\n",
    "  \"\"\",\n",
    "  input_variables=[\"information\"]\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "country_flag_colors_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "country_flag_colors_chain = capital_template | country_flag_colors_llm | output_parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34568108",
   "metadata": {},
   "source": [
    "## RunnableLambdaë¥¼ ì´ìš©í•œ ì¤‘ê°„ ê°’ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e118d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  information: PromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='\\n  Give following information about {country}:\\n  - Capital\\n  - Population\\n  - Language\\n  - Currency\\n\\n  return it in JSON format. and return the JSON dictionary only.\\n  ')\n",
       "               | RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x112117950>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x112116b10>, root_client=<openai.OpenAI object at 0x112117ce0>, root_async_client=<openai.AsyncOpenAI object at 0x112115a70>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')), kwargs={'response_format': <class '__main__.CountryDetail'>, 'ls_structured_output_format': {'kwargs': {'method': 'json_schema', 'strict': None}, 'schema': {'type': 'function', 'function': {'name': 'CountryDetail', 'description': '', 'parameters': {'properties': {'capital': {'description': 'The capital of the country', 'type': 'string'}, 'population': {'description': 'The population of the country', 'type': 'integer'}, 'language': {'description': 'The language of the country', 'type': 'string'}, 'currency': {'description': 'The currency of the country', 'type': 'string'}}, 'required': ['capital', 'population', 'language', 'currency'], 'type': 'object'}}}}}, config={}, config_factories=[])\n",
       "               | RunnableBinding(bound=RunnableLambda(...), kwargs={}, config={}, config_factories=[], custom_output_type=<class '__main__.CountryDetail'>)\n",
       "}\n",
       "| RunnableLambda(print_intermediate_output)\n",
       "| PromptTemplate(input_variables=['information'], input_types={}, partial_variables={}, template=\"\\n  guess country's flag colors.\\n  just return the colors.\\n\\n  # information\\n  {information}\\n  \")\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x112307110>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x112306fd0>, root_client=<openai.OpenAI object at 0x1123056d0>, root_async_client=<openai.AsyncOpenAI object at 0x112307390>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "def print_intermediate_output(answer):\n",
    "  print(\"ì¤‘ê°„ ê°’ ì¶œë ¥ =>\", answer)\n",
    "\n",
    "  return answer\n",
    "\n",
    "\n",
    "final_chain = {\"information\": country_llm_chain} | RunnableLambda(print_intermediate_output) |  country_flag_colors_chain \n",
    "final_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5715d90",
   "metadata": {},
   "source": [
    "## RunnablePassthroughë¥¼ ì´ìš©í•œ í¸í•œ ì…ë ¥ êµ¬í˜„\n",
    "\n",
    "ê·¸ëƒ¥ ë‹¨ì¼ê°’ì„ ë„£ì–´ì¤„ ë•ŒëŠ” keyê°’ì„ ìƒëµê°€ëŠ¥í•˜ì§€ë§Œ ë§Œì•½ country_llm_chainì´ `ë‘ ê°œ ì´ìƒ input_variables`ë¥¼ ë°›ëŠ”ë‹¤ë©´ key, valueë¥¼ ëª¨ë‘ ì‘ì„±í•´ì•¼ í•œë‹¤.\n",
    "\n",
    "```python\n",
    "final_chain = {\n",
    "  \"country\": RunnablePassthrough(),\n",
    "  \"continet\": RunnablePassthrough()\n",
    "} | country_llm_chain | RunnableLambda(print_intermediate_output) | country_flag_colors_chain \n",
    "\n",
    "answer = final_chain.invoke({\n",
    "  \"country\": \"France\",\n",
    "  \"continent\": \"Europe\"\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3058e08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "final_chain = {\"country\": RunnablePassthrough()} | country_llm_chain | RunnableLambda(print_intermediate_output) | country_flag_colors_chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba76a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¤‘ê°„ ê°’ ì¶œë ¥ => capital='Paris' population=65273511 language='French' currency='Euro'\n",
      "Blue, White, Red\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# RunnablePassthrough ì‚¬ìš© ì „\n",
    "# e.g. answer = final_chain.invoke({\"country\": \"France\"})\n",
    "answer = final_chain.invoke(\"France\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3496ce",
   "metadata": {},
   "source": [
    "# prompt ê¿€íŒ\n",
    "\n",
    "ì‚¬ëŒê³¼ ê°™ì´ llmì€ ë¬´ì—‡ì„ í•´ì•¼í•˜ê²Œë” ì„¤ê³„ê°€ ë˜ì–´ìˆëŠ”ë° ì‚¬ìš©ìë“¤ì€ `í•˜ì§€ë§ë¼ëŠ” ê²ƒ`ê³¼ `í•´ì•¼ í•˜ëŠ” ê²ƒ`ì„ ëª…ë ¹í•œë‹¤.  \n",
    "ë”°ë¼ì„œ, `í•˜ì§€ ë§ì•„ì•¼í•˜ëŠ” ê²ƒ(safety)`ì€ ë³„ë„ì˜ chainì„ êµ¬ì„±í•˜ê³  `pydantic`ê³¼ ê°™ì´ `binary result`ê°€ ë‚˜ì˜¤ê²Œë” êµ¬í˜„ì„ í•˜ê³  ë‚˜ì„œ `í†µê³¼ì‹œì—ë§Œ` ë‹¤ìŒ chainìœ¼ë¡œ ë„˜ì–´ê°€ê²Œ í•œë‹¤.\n",
    "\n",
    "ì´ë ‡ê²Œ í•˜ë©´ safety chainì€ ê°„ë‹¨í•œ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ë¯€ë¡œ ì ì€ ë¹„ìš©ì˜ ëª¨ë¸ì„ ì‚¬ìš©, ì´í›„ ì‹¤ì œ logicì´ ëŒì•„ê°€ëŠ” chainì€ ì„±ëŠ¥ì´ ì¤‘ìš”í•˜ê¸° ë•Œë¬¸ì— ê³  ë¹„ìš©ì˜ ëª¨ë¸ê³¼ ê°™ì´ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "```plaintext\n",
    "safety_chain(low cost model) ---- no -> âŒ\n",
    "             ã„´--- yes -> next_chain(high cost model) -> âœ…\n",
    "```\n",
    "\n",
    "_**<span style=\"color: orange;\">ë˜í•œ promptëŠ” í•˜ë‚˜ì˜ taskë¥¼ ì²˜ë¦¬í•˜ë„ë¡ í•˜ê³  ì´ë¥¼ ì¡°í•©í•˜ì—¬ chainì„ êµ¬ì„±í•˜ëŠ” ê²ƒì´ í•˜ë‚˜ì˜ Promptì— ì—¬ëŸ¬ ì§ˆë¬¸ì„ ë„£ì–´ ì²˜ë¦¬í•˜ëŠ” ê²ƒ ë³´ë‹¤ í›¨ì”¬ ì„±ëŠ¥ì´ ì¢‹ë‹¤ê³  í•œë‹¤.</span>**_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1823010e",
   "metadata": {},
   "source": [
    "# Knowledge cutoff\n",
    "\n",
    "llmì€ ê³¼ê±°ì˜ ë°ì´í„°ë¥¼ í•™ìŠµí•œ ì–¸ì–´ ëª¨ë¸ì´ë‹¤.\n",
    "- gpt-4oê°€ í•™ìŠµí•œ ë°ì´í„°ì˜ ë§ˆì§€ë§‰ ì‹œì ì„ `knowledge cutoff`ë¼ê³  í•œë‹¤.\n",
    "- ì¦‰, gpt-4oëŠ” 2023ë…„ 10ì›” ì´ì „ì˜ ë°ì´í„°ë§Œ í•™ìŠµí•œ ëª¨ë¸ì´ë‹¤.\n",
    "\n",
    "í•˜ì§€ë§Œ, `legal: ë²•`ê³¼ ê°™ì€ ë„ë©”ì¸ì— ëŒ€í•œ ì–¸ì–´ ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì€ ìƒˆë¡œìš´ ë²• ê°œì •ì•ˆì´ ë‚˜ì˜¬ ë•Œë§ˆë‹¤ í•™ìŠµì„ ì‹œì¼œì£¼ì–´ì•¼ í•œë‹¤. \n",
    "\n",
    "ë”°ë¼ì„œ, ìƒˆë¡œìš´ í•™ìŠµ ë°ì´í„°ë¥¼ ì‚¬ëŒì´ ì°¾ì•„ì„œ ì£¼ì–´ì•¼ í•˜ëŠ”ë° ì´ ë•Œ `RAG`ë¥¼ í™œìš©í•˜ë©´ í•´ê²°í•  ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "\n",
    "> ì‹œëŒ€ íë¦„:  `perception AI` -> `Generative AI` -> `Agent AI` -> `Robot`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72f5ce1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inflearn-rag-llm-application",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
