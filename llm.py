from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import (
    ChatPromptTemplate,
    MessagesPlaceholder,
    PromptTemplate,
)
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone
from dotenv import load_dotenv

load_dotenv()

# v1.0ìœ¼ë¡œ ë°”ë€Œë©´ì„œ ê¸°ì¡´ì˜ chat historyëŠ” langgraphì—ì„œ ì œê³µì´ ë¼ ì¼ë‹¨ì€ MessagesPlaceholderë¡œ êµ¬í˜„í•¨.
# chat historyì— ê´€ë ¨í•œ ì½”ë“œëŠ” ğŸ’¬ë¡œ ì£¼ì„ë‹´.
chat_history = []


def get_llm(model="gpt-5-mini"):
    llm = ChatOpenAI(model=model)

    return llm


def get_retriever():
    TAX_DB_NAME = "tax-markdown"
    embedding_function = OpenAIEmbeddings(model="text-embedding-3-large")
    pc = Pinecone()
    pinecone_db = PineconeVectorStore.from_existing_index(
        index_name=TAX_DB_NAME, embedding=embedding_function
    )
    retriever = pinecone_db.as_retriever()

    return retriever


def get_keyword_chain():
    llm = get_llm()

    keyword_mapping = """
      - ì—°ë´‰/ì´ìˆ˜ìµ/ì†Œë“(ìˆ˜ìµ)ê³¼ ê´€ë ¨ëœ í‘œí˜„ì„ í¬ê´„í•˜ëŠ” ë‹¨ì–´ -> "ì¢…í•©ì†Œë“"ìœ¼ë¡œ ë³€ê²½í•œë‹¤.
      - ì§ì¥ì¸/ì‚¬ëŒ(ë‚©ì„¸ì˜ë¬´ì) ë“± ì¸ì  ì£¼ì²´ë¥¼ ì˜ë¯¸í•˜ëŠ” ë‹¨ì–´ -> "ê±°ì£¼ì"ë¡œ ë³€ê²½í•œë‹¤.
    """

    # keyword mapping prompt
    keyword_prompt_content = f"""
      ë‹¹ì‹ ì€ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì „ë¬¸ê°€ì´ì ì¿¼ë¦¬ë¥¼ ì¬ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤..
      ì£¼ìš” ì„ë¬´ëŠ” ì‚¬ìš©ì ì§ˆì˜ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì •í™•íˆ ì¶”ì¶œí•˜ê³ , ì œê³µëœ #í‚¤ì›Œë“œ ë§¤í•‘ ê·œì¹™ì„ í†µí•´ 
      ì•„ë˜ í‚¤ì›Œë“œ ë§¤í•‘ ê·œì¹™ì„ ì°¸ê³ í•˜ì—¬ #ì‚¬ìš©ì ì¿¼ë¦¬ì˜ queryë¥¼ ì¬ì‘ì„±í•´ì£¼ì„¸ìš”.

      ë‹¨, ì•„ë˜ì˜ ê·œì¹™ì€ ì¤€ìˆ˜í•˜ì„¸ìš”.
      - queryë¥¼ ë³€ê²½í•  í•„ìš”ê°€ ì—†ë‹¤ë©´ ë³€ê²½í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.
      - ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ìœ ì§€í•œì±„ ë‹¨ì–´ë§Œ # í‚¤ì›Œë“œ ë§¤í•‘ ê·œì¹™ì„ ì°¸ê³ í•´ ë³€í™˜í•˜ë©´ ë©ë‹ˆë‹¤.

      #í‚¤ì›Œë“œ ë§¤í•‘ ê·œì¹™
      {keyword_mapping}

      #ì‚¬ìš©ì ì¿¼ë¦¬
      query: {{query}}
    """

    keyword_prompt = PromptTemplate.from_template(keyword_prompt_content)
    keyword_chain = (
        {"query": RunnablePassthrough()} | keyword_prompt | llm | StrOutputParser()
    )

    return keyword_chain


# invokeê°€ ì•„ë‹Œ streamì„ í†µí•œ ai message ìƒì„±ì€ generatorì´ê¸° ë•Œë¬¸ì—
# AiMessageì˜ contentëŠ” stringì„ ê¸°ëŒ€í•˜ì§€ë§Œ generatorë¥¼ ë°›ì•„ ì˜¤ë¥˜ê°€ ë‚¨ì„ í•´ê²°í•˜ê¸° ìœ„í•œ í•¨ìˆ˜
def get_string_from_stream(stream_response):
    result = ""

    for chunk in stream_response:
        result += chunk

    return result


def get_qa_chain():
    llm = get_llm()
    system_prompt = """
      ë‹¹ì‹ ì€ ìµœê³ ì˜ í•œêµ­ ì†Œë“ì„¸ ì „ë¬¸ê°€ ì…ë‹ˆë‹¤. #Contextë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”

      #Context
      {context}

      #User Query
      Question: {query}
    """

    # ğŸ’¬
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "Question: {query}"),
        ]
    )

    retriever = get_retriever()
    query_chain = (
        {
            "query": RunnablePassthrough(),
            "context": retriever,
            "chat_history": lambda x: chat_history,
        }
        | qa_prompt
        | llm
        | StrOutputParser()
    )

    return query_chain


def get_ai_message(query):
    keyword_chain = get_keyword_chain()
    qa_chain = get_qa_chain()
    final_chain = keyword_chain | qa_chain
    
    # ! gpt-5ì—ì„œëŠ” ì¡°ì§ ì¸ì¦ í›„ì— stream serviceë¥¼ ì´ìš©í•  ìˆ˜ ìˆì–´ ê·¸ëƒ¥ invokeë¡œ ëŒ€ì²´í•¨
    # ! ì¦‰ ë¬´ë£Œ ê³„ì •ì€ ì•ˆ ëœë‹¤.... ã…œã…œ
    # result = final_chain.stream(query)
    result = final_chain.invoke(query)

    # ğŸ’¬ historyì— ìµœê·¼ user, ai messageë¥¼ ë‹´ì•„ MessagesPlaceholderì— ë‹´ê¸°
    chat_history.append(HumanMessage(content=query))
    chat_history.append(AIMessage(content=result))

    print(chat_history)

    return result
