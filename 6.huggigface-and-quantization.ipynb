{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfcfe915",
   "metadata": {},
   "source": [
    "- HuggingFacePipeline: inference APIë¥¼ ì œê³µí•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì„ localì—ì„œ ë‹¤ìš´ë°›ì•„ ì‚¬ìš©\n",
    "- huggingFaceEndPoint: hoistingì„ ì œê³µí•˜ê¸° ë•Œë¬¸ì— ì„œë²„ì— ìˆëŠ” gpuë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬´ë£Œë¡œ ì‚¬ìš©(localì´ ì•„ë‹˜)\n",
    "\n",
    "ë§Œì•½ localì—ì„œ computing resourceê°€ llmì„ ê°ë‹¹í•˜ì§€ ëª» í•˜ëŠ” ì‚¬ì–‘ì´ë¼ë©´, hugging faceì—ì„œ ì œê³µí•˜ëŠ” `Dedicated endpoint(ì „ìš© ì—”ë“œí¬ì¸íŠ¸)`ë¥¼ í†µí•´ ë°°í¬ í›„ ì‚¬ìš©í•´ë³¼ ìˆ˜ ìˆë‹¤. -> [[ğŸ”— ë°°í¬ Link](https://huggingface.co/inference-endpoints/dedicated)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e83d412",
   "metadata": {},
   "source": [
    "# huggingface model load\n",
    "\n",
    "\n",
    "huggingfaceì—ì„œ modelì„ ë‹¤ìš´ë°›ì€ ê²½ë¡œë¥¼ ì•Œ ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "```python\n",
    "# from huggingface_hub import constants\n",
    "# print(constants.hf_cache_home)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbd64049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.26s/it]\n",
      "Device set to use mps:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "  model_id=\"microsoft/Phi-3-mini-4k-instruct\",  \n",
    "  task=\"text-generation\",\n",
    "  pipeline_kwargs={\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"do_sample\": False,\n",
    "    # \"top_k\": 50, // do_sample=Trueì¼ ë•Œ ê°€ëŠ¥í•œ ì˜µì…˜\n",
    "    # \"temperature\": 0.1, // do_sample=Trueì¼ ë•Œ ê°€ëŠ¥í•œ ì˜µì…˜\n",
    "    \"repetition_penalty\": 1.03\n",
    "  }\n",
    ")\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ee3f9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<|user|>\\nwhat is hugging face?<|end|>\\n<|assistant|>\\n Hugging Face is a company that provides AI models and tools for natural language processing (NLP) tasks. It was founded by Facebook AI researchers in 2018, with the goal of making state-of-the-art NLP models accessible to everyone. The company offers pre-trained models, fine-tuning capabilities, and APIs for various NLP tasks such as text generation, translation, summarization, and more.\\n\\nHugging Face's most popular model is GPT-3, which stands for Generative Pre-trained Transformer 3. This model has been trained on a diverse range of internet text data and can generate human-like text based on given prompts. Other notable models include BERT, RoBERTa, and T5.\\n\\nThe company also provides an open-source platform called the Hugging Face Hub, where users can share their own models, datasets, and code. This\", additional_kwargs={}, response_metadata={}, id='run--b3e3b738-d7cb-4ea7-a33b-389c4ee372f9-0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(\"what is hugging face?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e0102d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pgj/.cache/huggingface\n"
     ]
    }
   ],
   "source": [
    "# hugging faceì—ì„œ ë°›ì€ model ì„¤ì¹˜ ê²½ë¡œ\n",
    "from huggingface_hub import constants\n",
    "\n",
    "print(constants.hf_cache_home)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c57b5b",
   "metadata": {},
   "source": [
    "# ì–‘ìí™”(Quantization)\n",
    "\n",
    "## Window + Cudaì—ì„œ ì–‘ìí™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962147b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "  load_in_4bit=True, # ê°€ì¤‘ì¹˜ë¥¼ 4bit ì •ìˆ˜ë¡œ ì••ì¶•\n",
    "  bnb_4bit_quant_type=\"nf4\", # quantization error(ì–‘ìí™” ì˜¤ì°¨)ë¥¼ ì¤„ì´ëŠ” ê¸°ë²•\n",
    "  # ìˆœì „íŒŒì‹œ 4bitë¡œ ì €ì¥ëœ ê°€ì¤‘ì¹˜ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ í’€ì–´(Dequantize on the fly) `float16`ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ê³±ì…‰/ëˆ„ì ì„ float16ìœ¼ë¡œ ìˆ˜í–‰\n",
    "  bnb_4bit_compute_dtype=\"float16\",\n",
    "  # 1ì°¨ ì–‘ìí™”ì—ì„œ ìƒê¸´ ë³´ì¡° íŒŒë¼ë¯¸í„°ë“¤ì„ í•œ ë²ˆ ë” ì €ë¹„íŠ¸ë¡œ ì–‘ìí™”í•´ì„œ ì €ì¥(ì¼ì¢…ì˜ 2ë‹¨ê³„ ì••ì¶•)í•˜ì—¬ ë©”ëª¨ë¦¬/ëŒ€ì—­í­.\n",
    "  # ë©”ëª¨ë¦¬ ì ˆê°â†‘ í•˜ì§€ë§Œ, ë³µì› ì‹œ ë¯¸ì„¸í•œ ì¶”ê°€ ì˜¤ì°¨ê°€ ìƒê¸¸ ìˆ˜ ìˆë‹¤.\n",
    "  bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "quantized_llm = HuggingFacePipeline.from_model_id(\n",
    "  model_id=\"microsoft/Phi-3-mini-4k-instruct\",  \n",
    "  task=\"text-generation\",\n",
    "  pipeline_kwargs={\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"do_sample\": False,\n",
    "    \"repetition_penalty\": 1.03\n",
    "  },\n",
    "  model_kwargs={\n",
    "    \"quantization_config\": quantization_config\n",
    "  }\n",
    ")\n",
    "\n",
    "quantized_chat_model = ChatHuggingFace(llm=quantized_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab0b68b",
   "metadata": {},
   "source": [
    "## Mac + MPSì—ì„œ ì–‘ìí™”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b257b86",
   "metadata": {},
   "source": [
    "- bitsandbytes ê°™ì€ CUDA ì˜ì¡´ íŒ¨í‚¤ì§€ëŠ” ë§¥ì—ì„œ ë™ì‘ ì œí•œì´ ìˆì–´ ë‹¤ë¥¸ ë°©ë²•ì„ ê³ ë ¤í•´ì•¼ í•œë‹¤.  \n",
    "  => macì—ì„œ ì–‘ìí™”í•˜ê¸° [ğŸ”— Link](https://developer.apple.com/kr/videos/play/wwdc2025/298/?time=187)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cb8a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# macì—ì„œ mpsë¥¼ ì‚¬ìš©í•œ ì˜ˆì œ\n",
    "import torch\n",
    "from mlx_lm.convert import convert\n",
    "\n",
    "print(\"MPS available on this device =>\", torch.backends.mps.is_available())\n",
    "\n",
    "# projection layer & embedding layerëŠ” 6bit, ì–‘ìí™” ê°€ëŠ¥í•œ layerëŠ” 4bit, ì–‘ìí™” ë¶ˆê°€ëŠ¥ì€ False return\n",
    "def mixed_quantization(layer_path, layer):\n",
    "  if \"lm_head\" in layer_path or 'embed_tokens' in layer_path:\n",
    "    return {\"bits\": 6, \"group_size\": 64}\n",
    "  elif hasattr(layer, \"to_quantized\"):\n",
    "    return {\"bits\": 4, \"group_size\": 64}\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "# quantization ì§„í–‰\n",
    "convert(\n",
    "  hf_path=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "  mlx_path=\"./models/microsoft-Phi-3-mini-4k-instruct-mixed-4-6-bit\",\n",
    "  dtype=\"float16\",\n",
    "  quantize=True,\n",
    "  q_bits=4,\n",
    "  q_group_size=64,\n",
    "  quant_predicate=mixed_quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c58c74b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "\n",
      "\n",
      "# Answer\n",
      "Hugging Face is a company and community that provides state-of-the-art pre-trained models and tools for Natural Language Processing (NLP) and computer vision. It offers a wide range of pre-trained models that can be easily integrated into various applications, such as chatbots, text generation, and image recognition. The company also provides a platform for researchers and developers to collaborate and share their work in the field of AI.\n",
      "==========\n",
      "Prompt: 8 tokens, 159.601 tokens-per-sec\n",
      "Generation: 102 tokens, 177.417 tokens-per-sec\n",
      "Peak memory: 2.324 GB\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\"./models/microsoft-Phi-3-mini-4k-instruct-mixed-4-6-bit\")\n",
    "system_prompt = \"\"\"\n",
    "  You are an expert in information retrieval and summarization. \n",
    "  Your job is to read the provided text and produce a precise, faithful, and concise summary. \n",
    "  Prioritize the authorâ€™s main claim, key evidence, and conclusions. \n",
    "  Use plain English and avoid filler. \n",
    "  Do not invent facts that arenâ€™t present in the input.\n",
    "\"\"\"\n",
    "messages = [{\n",
    "  \"role\": \"system\", \"content\": system_prompt\n",
    "}]\n",
    "prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "# response = generate(model, tokenizer, prompt=\"what is hugging face?\", verbose=True)\n",
    "\n",
    "# Generate the response\n",
    "response = generate(model, tokenizer, prompt = \"what is the hugging face??\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ac455c",
   "metadata": {},
   "source": [
    "## langchainê³¼ ì—°ê³„\n",
    "\n",
    "mlx_lmì—ì„œ langchainì—ì„œëŠ” `generate_step`ì„ ì´ìš©í•˜ì—¬ formatterë¥¼ ì „ë‹¬í•˜ì§€ë§Œ mlx_lmì—ì„œëŠ” `generate_step`ì´ ì¡´ì¬í•˜ì§€ë§Œ mlx_lm.utilsì— ì¡´ì¬í•˜ì§€ ì•Šì•„ ì˜¤ë¥˜ê°€ ë°œìƒí•œë‹¤. ì•„ë§ˆ ë²„ì „ í˜¸í™˜ ë¬¸ì œì´ë©° ì´ëŠ” ê¸°ë‹¤ë ¤ì•¼í•  ê²ƒ ê°™ë‹¤. ë”°ë¼ì„œ ê° ë‹¨ê³„ë¥¼ `runnableLabmda`ë¡œ ë§Œë“¤ì–´ LCELë¬¸ë²• ê¸°ë°˜ chainì„ êµ¬ì„±í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf896eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:42<00:00,  6.04s/it]\n"
     ]
    }
   ],
   "source": [
    "from functools import wraps\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from mlx_lm import generate, load\n",
    "\n",
    "quantized_model_path = \"./models/microsoft-Phi-3-mini-4k-instruct-mixed-4-6-bit\"\n",
    "model, tokenizer = load(quantized_model_path)\n",
    "\n",
    "def runnable_wrapper(func):\n",
    "  \"\"\"RunnableLambda wrapper function\"\"\"\n",
    "  @wraps(func)\n",
    "  def wrapper(*args, **kwargs):\n",
    "    return RunnableLambda(func)\n",
    "  \n",
    "  return wrapper\n",
    "\n",
    "@runnable_wrapper\n",
    "def create_chat_prompt(question):\n",
    "  messages = [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"\"\"\n",
    "        You are an expert in information retrieval and summarization. \n",
    "        Your job is to read the provided text and produce a precise, faithful, and concise summary. \n",
    "        Prioritize the authorâ€™s main claim, key evidence, and conclusions. \n",
    "        Use plain English and avoid filler. \n",
    "        Do not invent facts that arenâ€™t present in the input.\n",
    "      \"\"\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": f\"\"\"\n",
    "        Question: {question}\n",
    "      \"\"\"\n",
    "    }\n",
    "  ]\n",
    "\n",
    "  prompt_without_tokenized = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "  prompt_with_tokenized = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "  print(\"ìƒì„±ëœ promptğŸ‘‡\\n\", prompt_without_tokenized)\n",
    "\n",
    "  return prompt_with_tokenized\n",
    "\n",
    "@runnable_wrapper\n",
    "def run_llm_with_mlx(prompt):\n",
    "  return generate(model=model, tokenizer=tokenizer, prompt=prompt)\n",
    "\n",
    "@runnable_wrapper\n",
    "def output_parser(answer):\n",
    "  return answer.replace(\"<|end|>\", \"\")\n",
    "  \n",
    "chain = create_chat_prompt() | run_llm_with_mlx() | output_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "426031e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of South Korea is Seoul.<|end|>'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"what is capital of south korea?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c5af3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 13 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 168811.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "\n",
      "\n",
      "# Answer\n",
      "Hugging Face is a company that specializes in creating and providing AI models and tools for natural language processing (NLP). They offer a wide range of pre-trained models, libraries, and frameworks that can be used for various NLP tasks such as text generation, translation, summarization, and more. Hugging Face's popular library, Transformers, provides a collection of state-of-the-art models that can be easily integrated into applications and projects. The company also focuses on promoting open-source collaboration and community engagement in the field of AI and NLP.\n",
      "==========\n",
      "Prompt: 8 tokens, 91.062 tokens-per-sec\n",
      "Generation: 132 tokens, 60.029 tokens-per-sec\n",
      "Peak memory: 15.284 GB\n"
     ]
    }
   ],
   "source": [
    "# bitsandbytes ê°™ì€ CUDA ì˜ì¡´ íŒ¨í‚¤ì§€ëŠ” ë§¥ì—ì„œ ë™ì‘ ì œí•œì´ í½ë‹ˆë‹¤\n",
    "# https://developer.apple.com/kr/videos/play/wwdc2025/298/?time=187\n",
    "\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "# hubì—ì„œ ê´€ë ¨ modelì„ localì— ë‹¤ìš´, pathëŠ” ì•„ë˜ ì½”ë“œë¥¼ í†µí•´ í™•ì¸\n",
    "# from huggingface_hub import constants\n",
    "# print(constants.hf_cache_home)\n",
    "model, tokenizer = load(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "# TODO: ì˜¬ë¦´ ë• ë¹¼ê¸°\n",
    "# print(model) ğŸ‘‰ğŸ» modelì— ê´€ë ¨í•˜ì—¬ layerë“¤ì„ ë³´ì—¬ì¤€ë‹¤\n",
    "# print(model.parameters()) ğŸ‘‰ğŸ» weight, biasë“± ê´€ë ¨ëœ parameterì— ëŒ€í•œ ì •ë³´\n",
    "# print(model.layers[0].self_attn) ğŸ‘‰ğŸ» íŠ¹ì • layerì—ë„ ì ‘ê·¼ì´ ê°€ëŠ¥í•˜ë‹¤.\n",
    "\n",
    "\n",
    "# Prepare the prompt for the model\n",
    "# messages = [{\n",
    "#   \"role\": \"system\", \"content\": \"You are an expert in information retrieval and summarization. Your job is to read the provided text and produce a precise, faithful, and concise summary. Prioritize the authorâ€™s main claim, key evidence, and conclusions. Use plain English and avoid filler. Do not invent facts that arenâ€™t present in the input.\"\n",
    "# },{\n",
    "#   \"role\": \"user\", \"content\": \"Write one paragraph that summarizes its core message and Provide an unordered list of three key related points\"\n",
    "# }]\n",
    "messages = [{\n",
    "  \"role\": \"system\", \"content\": \"You are an expert in information retrieval and summarization. Your job is to read the provided text and produce a precise, faithful, and concise summary. Prioritize the authorâ€™s main claim, key evidence, and conclusions. Use plain English and avoid filler. Do not invent facts that arenâ€™t present in the input.\"\n",
    "}]\n",
    "prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "# response = generate(model, tokenizer, prompt=\"what is hugging face?\", verbose=True)\n",
    "\n",
    "# Generate the response\n",
    "response = generate(model, tokenizer, prompt = \"what is the hugging face??\", verbose=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adc95cf",
   "metadata": {},
   "source": [
    "# ì˜¤ë¥˜ ì½”ë“œ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a972f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.mlx_pipeline import MLXPipeline\n",
    "from mlx_lm import load\n",
    "\n",
    "model_path = \"./models/microsoft-Phi-3-mini-4k-instruct-mixed-4-6-bit\"\n",
    "model, tokenizer = load(model_path)\n",
    "\n",
    "llm = MLXPipeline(\n",
    "  model=model,\n",
    "  tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "llm.invoke(\"what's my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb80f7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quantization \"command\" and \"python-api\"\n",
    "\n",
    "# Quantization of command in cli\n",
    "mlx_lm.convert --hf-path \"mistralai/Mistral-7B-Instruct-v0.3\" \\ ğŸ‘‰ğŸ» ëª¨ë¸ ìœ„ì¹˜(ì¶œì²˜)\n",
    "               --mlx-path \"./mistral-7b-v0.3-4bit\" \\ ğŸ‘‰ğŸ» í™˜/ì–‘ìí™”ê°€ ëë‚œ MLX ëª¨ë¸ì˜ ì €ì¥ ìœ„ì¹˜\n",
    "               --dtype float16 \\ ğŸ‘‰ğŸ» ì¶”ë¡ (ê³„ì‚°; dequantize on-the-fly) ì‹œ  í†µí•œ data typeì§€ì •\n",
    "               --quantize --q-bits 4 --q-group-size 64 ğŸ‘‰ğŸ» --q-bits 4: ì–‘ìí™”ì‹œ 4bitë¡œ ì–‘ìí™” ì§„í–‰ / --q-group-size: ê°€ì¤‘ì¹˜ í…ì„œì˜ íŠ¹ì • ì¶•ì„ ë”°ë¼ â€˜ëª‡ ê°œâ€™ì”© ê°™ì€ ìŠ¤ì¼€ì¼ì„ ê³µìœ í• ì§€ ì •í•˜ëŠ” ê°’\n",
    "               --upload-repo \"my-name/mistral-7b-v0.3-4bit\"   ğŸ‘‰ğŸ» hugging faceì— í˜¸ìŠ¤íŒ… ëœ modelì„ ë‹¤ë¥¸ ì‚¬ëŒë“¤ê³¼ ê³µìœ \n",
    "mlx_lm.convert --hf-path \"mistralai/Mistral-7B-Instruct-v0.3\" \\ \n",
    "               --mlx-path \"./mistral-7b-v0.3-4bit\" \\ \n",
    "               --dtype float16 \\ \n",
    "               --quantize --q-bits 4 --q-group-size 64 \n",
    "               --upload-repo \"my-name/mistral-7b-v0.3-4bit\"   \n",
    "\n",
    "# what is tensor in deep learning -> https://towardsdatascience.com/what-is-a-tensor-in-deep-learning-6dedd95d6507/\n",
    "\n",
    "# 3) ì–´ë–»ê²Œ ì„ íƒí•˜ë‚˜ìš”? (ì‹¤ì „ ê°€ì´ë“œ)\n",
    "# ì²˜ìŒ ì‹œì‘: --q-group-size 64 ê¶Œì¥(ê¸°ë³¸ê°’ì¸ ê²½ìš°ê°€ ë§ìŒ).\n",
    "# í’ˆì§ˆì´ ì•„ì‰½ë‹¤(íŠ¹íˆ ìˆ˜í•™/ì½”ë”©/ì¥ë¬¸ í’ˆì§ˆ): 32ë¡œ ë‚®ì¶° í’ˆì§ˆâ†‘(ì˜¤ë²„í—¤ë“œâ†‘)\n",
    "# ë©”ëª¨ë¦¬ê°€ ë” ê¸‰í•˜ë‹¤: 128ë¡œ ë†’ì—¬ ì €ì¥ëŸ‰/ëŒ€ì—­í­â†“(í’ˆì§ˆ ë¦¬ìŠ¤í¬â†‘)\n",
    "# ë¯¼ê° ë ˆì´ì–´(ì…ì¶œë ¥ í”„ë¡œì ì…˜, LayerNorm ì£¼ë³€ ë“±)ëŠ” ê·¸ë£¹ì„ ë” ì´˜ì´˜(32)í•˜ê²Œ í•˜ê±°ë‚˜ í•´ë‹¹ ë ˆì´ì–´ë§Œ FP16 ìœ ì§€(í˜¼í•© ì •ë°€)ë„ ì‹¤ì „ì—ì„œ ìì£¼ ì”ë‹ˆë‹¤.\n",
    "# í•œ ì¤„ ìš”ì•½: 64ëŠ” â€œì •í™•ë„ ì†ì‹¤ì„ í¬ê²Œ ëŠ˜ë¦¬ì§€ ì•Šìœ¼ë©´ì„œ ë©”íƒ€ë°ì´í„°ì™€ ì»¤ë„ íš¨ìœ¨ì„ ì˜ ë§ì¶˜â€ ì‹¤ì „í˜• ê¸°ë³¸ê°’ì…ë‹ˆë‹¤.\n",
    "\n",
    "# Quantization with python api\n",
    "from mlx_lm.convert import convert\n",
    "\n",
    "# We can choose a different quantization per layer\n",
    "def mixed_quantization(layer_path, layer, model_config):\n",
    "  # projection layer, embedding layerëŠ” ë¯¼ê°í•˜ê¸° ë•Œë¬¸ì— ì–‘ìí™”í•˜ëŠ” bitë³´ë‹¤ ë„ˆ ë†’ì€ bitë¡œ í• ë‹¹í•œë‹¤.\n",
    "  if \"lm_head\" in layer_path or \"embed_tokens\" in layer_path:\n",
    "    return {\"bits\": 6, \"group_size\": 64}\n",
    "  # ì–‘ìí™” ì‹œ quantizationì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ìˆìœ¼ë¯€ë¡œ ì§€ì›í•˜ëŠ” ê²½ìš°ë§Œ ì–‘ìí™”ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆë‹¤.\n",
    "  elif hasattr(layer, \"to_quantized\"):\n",
    "    return {\"bits\": 4, \"group_size\": 64}\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "# Convert can be used to change precision, quantize and upload models to Hugging Face\n",
    "convert(\n",
    "  hf_path=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "  mlx_path=\"./mistral-7b-v0.3-mixed-4-6-bit\",\n",
    "  quantize=True,\n",
    "  quant_predicate=mixed_quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65728b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pgj/.cache/huggingface\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import constants\n",
    "print(constants.hf_cache_home)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f9cc19",
   "metadata": {},
   "source": [
    "# Fine-Tuning with MLX-LM\n",
    "mlx-lmì—ì„œëŠ” `ë‘ ê°€ì§€ typ`ì„ ì§€ì›í•œë‹¤.\n",
    "\n",
    "- `full model fine-tuning`: ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì˜ ëª¨ë“  parametersë¥¼ updateí•œë‹¤.\n",
    "- `Low-Rank Adapter training`: ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì˜ weightsë“¤ì€ `freeze`í•˜ê³  ì¶”ê°€ parametersë¥¼ í•™ìŠµì‹œì¼œ `ê¸°ì¡´ íŒŒë¼ë¯¸í„°ì— ë”í•´ì ¸ ê²°ê³¼ì ìœ¼ë¡œëŠ” ê°€ì¤‘ì¹˜ê°€ ë°”ë€ ê²ƒì²˜ëŸ¼ ë™ì‘`í•œë‹¤. ë”°ë¼ì„œ, localì—ì„œ fine-tuningì„ ì§„í–‰í•˜ê³ ì í•  ë•Œ ë©”ëª¨ë¦¬ ì‚¬ìš©ì´ ì ê³  í›¨ì”¬ ë¹ ë¥´ê¸°ì— localì—ì„œ fine-tuningì „ëµìœ¼ë¡œ ì ì ˆí•˜ë‹¤.\n",
    "\n",
    "fine-tuningì„ í•  ë•Œ fine-grainedë¡œ tuningì„ í•˜ê³  ì‹¶ì„ ë• [[ğŸ”— lora_config.yaml](https://github.com/ml-explore/mlx-examples/blob/9000e280aeb56c2bcce128001ab157030095687a/llms/mlx_lm/examples/lora_config.yaml#L17)]ì„ í†µí•´ì„œ ì‰½ê²Œ ì„¤ì •ì„ í•  ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "\n",
    "ğŸ‘‰ğŸ» fine-tuning command\n",
    "```shell\n",
    "$) mlx_lm.lora --train \\\n",
    "            --model \"mlx-community/Mistral-7B-Instruct-v0.3-4bit\" \\\n",
    "            --data \"path/to/our/data/folder\" \\\n",
    "            --iters 400 \\\n",
    "            --batch-size 16\n",
    "```\n",
    "\n",
    "fine-tuningì„ í•˜ê¸° ìœ„í•´ ì•„ë˜ì˜ ê³¼ì •ë“¤ì„ ë³´ì(Loraë¥¼ í†µí•œ `row-rank adapter fine-tuning`)\n",
    "\n",
    "commandë¥¼ í†µí•´ ìµœê·¼ì— Super Bowlì—ì„œ ëˆ„ê°€ ì´ê²¼ëƒëŠ” ì§ˆë¬¸ì— ë‹µì€ ë§ì§€ë§Œ ìµœê·¼ ì •ë³´ë¡œ updateë˜ì§€ ì•Šì•„ ìµœê·¼ ì •ë³´ë¥¼ í†µí•´ì„œ `fine-tuning`ì´ í•„ìš”í•œ ìƒí™©ì´ë‹¤.\n",
    "\n",
    "```shell\n",
    "$) mlx_lm.generate --model mlx-community/Mistral-7B-Instruct-v0.3-4bit --prompt 'Who won the latest Super Bowl'\n",
    "```\n",
    "\n",
    "`knowledge cutoff`ì´í›„ì˜ super bowlì— ëŒ€í•œ data(`question-answer`)ë¥¼ í•™ìŠµì‹œì¼œ modelì„ updateí•  ìˆ˜ ìˆë‹¤. ì•„ë˜ linkì—ì„œ `jsonl`ì˜ êµ¬ì¡°ë¡œ `í‘œì˜ í˜•íƒœ`ë¥¼ í•˜ê³  ìˆë‹¤. í•˜ë‚˜ì˜ rowë¥¼ train dataë¡œ ì£¼ì…ì„ í•´ì•¼ í•˜ëŠ”ë° nlpëŠ” ë¬¸ìì—´ì„ í†µí•´ í•™ìŠµì„ ì§„í–‰í•˜ë¯€ë¡œ `table:, columns:, Q:, A:`ì™€ ê°™ì´ `label`ì„ ì£¼ì–´ modelì´ ì—­í•  íŒíŠ¸ë¥¼ ì£¼ì–´ `prompt`ì™€ ê°™ì´ ì œê³µì„ í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ëª¨ë¸ ì…ë ¥ì€ ë¬¸ìì—´ì´ê¸° ë•Œë¬¸ì— ì´ëŸ¬í•œ êµ¬ì¡°ë¥¼ í†µí•´ì„œ train dataë¥¼ êµ¬ì„±í•œë‹¤.\n",
    "=> [[ğŸ”— train data link](https://github.com/ml-explore/mlx-examples/tree/main/lora/data)]\n",
    "\n",
    "```shell\n",
    "$) mlx_lm.lora --train \\\n",
    "               --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \\\n",
    "               --data ./data \\\n",
    "               --iters 300 \\\n",
    "               --batch-size 8 \\\n",
    "               --mask-prompt \\\n",
    "               --learning-rate 1e-5\n",
    "```\n",
    "\n",
    "ì´ë ‡ê²Œ fine-tuningì„ ì§„í–‰í•˜ë©´ loraë¥¼ í†µí•´ì„œ adapterê°€ ìƒì„±ëœë‹¤. ì´ `adapter`ë¥¼ ì´ìš©í•˜ì—¬ ì¶”ê°€ì ìœ¼ë¡œ ìƒì„±ëœ low rank parameterë¥¼ ì¶”ê°€í•˜ì—¬ ë‹µë³€ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.\n",
    "```shell\n",
    "$) mlx_lm.generate --model mlx-community/Mistral-7B-Instruct-v0.3-4bit --prompt \"Who won the latest Super Bowl\" --adapter adapters\n",
    "```\n",
    "\n",
    "ì´ì œ `ìµœê·¼ì— super bowl`ì—ì„œ ì´ê¸´ íŒ€ì„ ì •í™•íˆ êµ¬í•  ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "> ğŸ’¡ ì—¬ê¸°ì„  `adapter`ëŠ” fine-tuningì„ í†µí•´ ë‚˜ì˜¨ `LoRA ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ/ID`ì´ë‹¤. ì¦‰ ì¶”ê°€ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ì´ì–´ì„œ fine-tuningì„ ì§„í–‰í•˜ì—¬ parameterë¥¼ updateí•  ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "ì§€ê¸ˆê¹Œì§€ ë³´ë©´ `adapter`ì™€ pre-trainedëœ ê°€ì¤‘ì¹˜ê°€ ë³„ë„ë¡œ ì¡´ì¬í•˜ëŠ”ë° ì´ë¥¼ `fuse`ë¥¼ í†µí•´ì„œ í•˜ë‚˜ì˜ modelë¡œ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "```shell\n",
    "mlx_lm.fuse --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \\\n",
    "            --adapter-path \"path/to/trained/adapters\" \\\n",
    "            --save-path \"fused-mistral-7B-Instruct-v0.3-4bit\" \\\n",
    "            --upload-repo \"my-name/fused-mistral-7B-Instruct-v0.3-4bit\" ğŸ‘‰ğŸ» í•´ë‹¹ ì˜µì…˜ì€ Hugging Faceì— í˜¸ìŠ¤íŒ… í•˜ëŠ” ê²ƒìœ¼ë¡œ ê³µìœ í•˜ê³ ì í•  ë•Œ ë„£ìœ¼ë©´ ë˜ëŠ” ì˜µì…˜ì´ë‹¤.\n",
    "```\n",
    "\n",
    "\n",
    "# fine tuning from gpt answer\n",
    "## 1) ìƒˆë¡œ LoRA í•™ìŠµ ì‹œì‘ (ê¸°ì¡´ ì–´ëŒ‘í„° ì—†ìŒ)\n",
    "\n",
    "```shell\n",
    "mlx_lm.lora --train \\\n",
    "  --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \\\n",
    "  --data ./data --iters 300 --batch-size 8 --learning-rate 1e-5 \\\n",
    "  --mask-prompt\n",
    "```\n",
    "\n",
    "> ğŸ’¡ í•™ìŠµì´ ëë‚˜ë©´ ì–´ëŒ‘í„°(LoRA) ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í„°ë¦¬ê°€ ìƒì„±ë¨\n",
    "\n",
    "## 2) ì´ì „ LoRAë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³„ì† í•™ìŠµ(ì§ˆë¬¸í•˜ì‹  ì¼€ì´ìŠ¤)\n",
    "\n",
    "ê°™ì€ ë² ì´ìŠ¤ ëª¨ë¸ ìœ„ì—ì„œ, ê³¼ê±°ì— ë§Œë“  ì–´ëŒ‘í„°ë¥¼ ë¶ˆëŸ¬ì™€ ì´ì–´ í•™ìŠµí•˜ë ¤ë©´:\n",
    "\n",
    "```shell\n",
    "mlx_lm.lora --train \\\n",
    "  --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \\\n",
    "  --adapter ./adapters/mistral-7b-lora  # â† ê¸°ì¡´ LoRA ë””ë ‰í„°ë¦¬ or HF repo\n",
    "  --data ./data_more --iters 200 --batch-size 8 --learning-rate 5e-6 \\\n",
    "  --mask-prompt\n",
    "```\n",
    "\n",
    "ì´ë ‡ê²Œ í•˜ë©´ ì›ë³¸ ê°€ì¤‘ì¹˜(ë² ì´ìŠ¤)ëŠ” freeze ìœ ì§€, ì–´ëŒ‘í„° íŒŒë¼ë¯¸í„°ë§Œ ì¶”ê°€ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n",
    "> ğŸ’¡ ì£¼ì˜: ì–´ëŒ‘í„°ëŠ” í•™ìŠµ ë‹¹ì‹œì˜ ë² ì´ìŠ¤ ì²´í¬í¬ì¸íŠ¸ì™€ ë™ì¼í•œ ë² ì´ìŠ¤ì— ì–¹ì–´ì•¼ í•©ë‹ˆë‹¤(ë™ì¼ ë²„ì „/ì–‘ìí™” ì„¤ì •). ë‹¤ë¥´ë©´ ê°€ì¤‘ì¹˜ ëŒ€ì‘ì´ ê¹¨ì§‘ë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "## 3) ì¶”ë¡ ì—ì„œ LoRA ì ìš©\n",
    "```shell\n",
    "mlx_lm.generate \\\n",
    "  --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \\\n",
    "  --adapter ./adapters/mistral-7b-lora \\\n",
    "  --prompt \"Explain LoRA in simple terms.\"\n",
    "```\n",
    "\n",
    "## 4) ë¨¸ì§€(merge)í•´ì„œ ë‹¨ì¼ ëª¨ë¸ë¡œ ë‚´ë³´ë‚´ê¸°(ì„ íƒ)\n",
    "\n",
    "ì–´ëŒ‘í„°ë¥¼ ë² ì´ìŠ¤ì— í•©ì³ ë‹¨ì¼ ì²´í¬í¬ì¸íŠ¸ë¡œ ë§Œë“¤ ë•Œ:\n",
    "\n",
    "```shell\n",
    "mlx_lm.merge \\\n",
    "  --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \\\n",
    "  --adapter ./adapters/mistral-7b-lora \\\n",
    "  --out ./merged-mistral7b\n",
    "```\n",
    "\n",
    "\n",
    "ë¨¸ì§€í•˜ë©´ `ì‹¤ì œ ê°€ì¤‘ì¹˜` ğ‘Š^ = ğ‘Š + Î”ğ‘Šë¡œ ì €ì¥ë˜ì–´ ì¶”ë¡  ì‹œ --adapter ì—†ì´ë„ ì‚¬ìš© ê°€ëŠ¥.\n",
    "> ğŸ’¡ ë‹¨, ì–‘ìí™” ë² ì´ìŠ¤(int4) ìœ„ì—ì„œ í•™ìŠµí•œ LoRAë¥¼ ë¨¸ì§€í•  ìˆ˜ ìˆëŠ”ì§€ëŠ” íˆ´ ë²„ì „ì— ë”°ë¼ ì œì•½ì´ ìˆì„ ìˆ˜ ìˆì–´, ë³´í†µì€ ì–´ëŒ‘í„°ë¡œ ìœ ì§€í•˜ê³  ëŸ°íƒ€ì„ í•©ì„±ì´ ì¼ë°˜ì ì…ë‹ˆë‹¤.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c306220f",
   "metadata": {},
   "source": [
    "# Quantization Fundamentals with Hugging Face From \"DeepLearning.AI\"\n",
    "\n",
    "free course => https://www.deeplearning.ai/short-courses/quantization-fundamentals-with-hugging-face/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc59d9d",
   "metadata": {},
   "source": [
    "# Lecture 1\n",
    "\n",
    "- LLM is so huge that hard to run on consumer grade hardware\n",
    "  => LLMì€ ë„ˆë¬´ ê±°ëŒ€í•´ì„œ ì¼ë°˜ ì†Œë¹„ììš© í•˜ë“œì›¨ì–´ì—ì„œ ëŒë¦¬ê¸° í˜ë“¤ë‹¤.\n",
    "\n",
    "ì´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ê²ƒì´ quantization(ì–‘ìí™”)ì´ë©° ë‹¤ì–‘í•œ data type option(int8, float16, bfloat16)ì„ í™œìš©í•˜ì—¬ modelì„ ì••ì¶•í•  ìˆ˜ ìˆë‹¤. 32bit ë¶€ë™ì†Œìˆ˜ì (float)ë¥¼ ì €ì¥í•˜ê³  ì••ì¶•í•˜ëŠ” ë°©ë²•ê³¼ ê¸°ìˆ ì ì¸ ì„¸ë¶€ì‚¬í•­ì„ ì ‘ëª©ì‹œí‚¨ë‹¤. \n",
    "\n",
    "quantizationì´ ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€ë¥¼ small-text generation modelì— hugging faceì˜ Quanto library(linear quantizationì„ ì–´ë–¤ pytorch modelì—ì„œ ì‚¬ìš©í•˜ê¸° ì‰½ê²Œ ë§Œë“  library)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°ìš¸ ê²ƒì´ë‹¤. \n",
    "\n",
    "transformer libraryë¥¼ ì´ìš©í•˜ì—¬ modelì„ loadí•˜ê³  quantization(ì–‘ìí™”)ì„ ì§„í–‰í•˜ê¸° ìœ„í•´ Quanto libraryë¥¼ ì‚¬ìš©í•œë‹¤.\n",
    "\n",
    "# Lecture2\n",
    "modelë“¤ì´ ê±°ëŒ€í•´ì§ì— ë”°ë¼ quantization(ì–‘ìí™”)ë¥¼ í†µí•´ modelì„ ì¶•ì†Œí•˜ì—¬ ì„±ëŠ¥ì €í•˜(with no little performance gradation)ì—†ì´ localì—ì„œ ëŒë¦´ ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "AI modelë“¤ì˜ í¬ê¸°ê°€ ê±°ëŒ€í•´ì§ì— ë¹„í•´ hardwareì—ì„œ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì •ë„ì˜ í¬ê¸°ì™€ ì ì  ê´´ë¦¬ê°€ ë°œìƒí•œë‹¤. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6265f34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iinfo(min=0, max=255, dtype=uint8)\n",
      "iinfo(min=-128, max=127, dtype=int8)\n",
      "iinfo(min=-9.22337e+18, max=9.22337e+18, dtype=int64)\n",
      "iinfo(min=-2.14748e+09, max=2.14748e+09, dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Information of `8-bit unsigned integer`\n",
    "print(torch.iinfo(torch.uint8))\n",
    "\n",
    "# Information of `8-bit (signed) integer`\n",
    "print(torch.iinfo(torch.int8))\n",
    "\n",
    "# Information of `64-bit (signed) integer`\n",
    "print(torch.iinfo(torch.int64))\n",
    "\n",
    "# Information of `32-bit (signed) integer`\n",
    "print(torch.iinfo(torch.int32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8fb79f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inflearn-rag-llm-application",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
